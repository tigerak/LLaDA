import json

# torch
import torch
from torch.utils.data import Dataset

class LLaDA_Dataset(Dataset):
    def __init__(self, json_path, tokenizer):

        with open(json_path, 'r', encoding="utf-8") as file:
            self.llada_data = json.load(file)

        self.tokenizer = tokenizer
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def __len__(self):
        return len(self.llada_data)
    
    def __getitem__(self, index):
        title = self.llada_data[index]["metadata"]["title"]
        article = self.llada_data[index]["document"]

        prompt_format = f"### TITLE: {title}\n### ARTICLE:"
        prompt_complete = f"{prompt_format} {article}"

        encoding = self.tokenizer(text=prompt_complete, 
                                  truncation=True,
                                  max_length=128,
                                  padding="max_length",
                                  add_special_tokens=True,
                                  return_tensors="pt")
        
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)

        prompt_enc = self.tokenizer(text=prompt_format, add_special_tokens=False)
        prompt_len = len(prompt_enc["input_ids"])

        masked_input_ids, mask_indices, t = mask_tokens_with_random_t(input_ids, 
                                                                   prompt_len, 
                                                                   self.tokenizer)

        labels = torch.full_like(input_ids, fill_value=-100)  # ì²˜ìŒë¶€í„° -100ìœ¼ë¡œ ì´ˆê¸°í™”
        labels[mask_indices] = input_ids[mask_indices]  # ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„ë§Œ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©
        
        return {"input_ids": masked_input_ids,
                "attention_mask": attention_mask,
                "labels": labels,
                "t": torch.Tensor(t)}
    
@staticmethod
def collate_fn(batch):
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    labels = torch.stack([item["labels"] for item in batch])
    t = torch.tensor([item["t"] for item in batch])

    return {"input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "t": t}

def mask_tokens_with_random_t(input_ids, 
                              prompt_len,
                              tokenizer,
                              max_t=1.0,
                              min_t=0.1):
    """
    - `t`ë¥¼ [min_t, max_t] ë²”ìœ„ì—ì„œ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•˜ì—¬ ê° ìƒ˜í”Œë§ˆë‹¤ ë‹¤ë¥¸ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ ì ìš©
    """

    seq_len = input_ids.shape[0]
    
    # Monte Carlo ë°©ì‹ìœ¼ë¡œ `t`ë¥¼ ìƒ˜í”Œë§
    t = torch.rand(1).item() * (max_t - min_t) + min_t  # 0.1~1.0 ì‚¬ì´ ëœë¤í•œ float ê°’

    # ğŸ”¹ í”„ë¡¬í”„íŠ¸ & íŠ¹ìˆ˜ í† í° ì œì™¸í•œ ë§ˆìŠ¤í‚¹ ê°€ëŠ¥í•œ í† í° ì°¾ê¸°
    valid_tokens = torch.ones(seq_len, dtype=torch.bool)  # ëª¨ë“  ìœ„ì¹˜ë¥¼ Trueë¡œ ì´ˆê¸°í™”
    valid_tokens[:prompt_len] = False  # í”„ë¡¬í”„íŠ¸ ì œì™¸
    valid_tokens[input_ids == tokenizer.pad_token_id] = False  # íŒ¨ë”© ì œì™¸
    valid_tokens[input_ids == tokenizer.bos_token_id] = False  # BOS ì œì™¸

    # `t` í™•ë¥ ë¡œ ë§ˆìŠ¤í‚¹ ì—¬ë¶€ ê²°ì •
    valid_indices = valid_tokens.nonzero(as_tuple=True)[0]  # ë§ˆìŠ¤í‚¹ ê°€ëŠ¥í•œ ì¸ë±ìŠ¤
    num_to_mask = max(1, round(t * valid_indices.shape[0]))  # ìµœì†Œ 1ê°œ ì´ìƒ ë§ˆìŠ¤í‚¹

    # num_to_mask ê°œìˆ˜ë§Œí¼ ì„ íƒí•˜ì—¬ ë§ˆìŠ¤í‚¹
    mask_indices = valid_indices[torch.randperm(len(valid_indices))[:num_to_mask]]

    # ë§ˆìŠ¤í‚¹ ì ìš©
    masked_input_ids = input_ids.clone()
    masked_input_ids[mask_indices] = tokenizer.mask_token_id  # `[MASK]` ì ìš©
    
    return masked_input_ids, mask_indices, t
    